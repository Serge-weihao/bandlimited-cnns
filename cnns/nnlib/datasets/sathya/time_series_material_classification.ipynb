{
  "cells": [],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import shutil, os, csv, itertools, glob\\n\",\n    \"\\n\",\n    \"import math\\n\",\n    \"import numpy as np\\n\",\n    \"import torch\\n\",\n    \"import torch.nn as nn\\n\",\n    \"import torch.nn.functional as F\\n\",\n    \"from torch.autograd import Variable\\n\",\n    \"from torch.utils.data import Dataset, DataLoader\\n\",\n    \"import torch.optim as optim\\n\",\n    \"\\n\",\n    \"from sklearn.metrics import confusion_matrix\\n\",\n    \"\\n\",\n    \"import pandas as pd\\n\",\n    \"import pickle as pk\\n\",\n    \"\\n\",\n    \"cuda \u003d torch.cuda.is_available()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"code_folding\": [\n     2,\n     19,\n     24,\n     30\n    ]\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Utils\\n\",\n    \"\\n\",\n    \"def load_pickle(filename):\\n\",\n    \"    try:\\n\",\n    \"        p \u003d open(filename, \u0027r\u0027)\\n\",\n    \"    except IOError:\\n\",\n    \"        print(\\\"Pickle file cannot be opened.\\\")\\n\",\n    \"        return None\\n\",\n    \"    try:\\n\",\n    \"        picklelicious \u003d pk.load(p)\\n\",\n    \"    except ValueError:\\n\",\n    \"        print(\u0027load_pickle failed once, trying again\u0027)\\n\",\n    \"        p.close()\\n\",\n    \"        p \u003d open(filename, \u0027r\u0027)\\n\",\n    \"        picklelicious \u003d pk.load(p)\\n\",\n    \"\\n\",\n    \"    p.close()\\n\",\n    \"    return picklelicious\\n\",\n    \"\\n\",\n    \"def save_pickle(data_object, filename):\\n\",\n    \"    pickle_file \u003d open(filename, \u0027w\u0027)\\n\",\n    \"    pk.dump(data_object, pickle_file)\\n\",\n    \"    pickle_file.close()\\n\",\n    \"    \\n\",\n    \"def read_data(filename):\\n\",\n    \"    print(\\\"Loading Data...\\\")\\n\",\n    \"    df \u003d pd.read_csv(filename, header\u003dNone)\\n\",\n    \"    data \u003d df.values\\n\",\n    \"    return data\\n\",\n    \"\\n\",\n    \"def read_line(csvfile, line):\\n\",\n    \"    with open(csvfile, \u0027r\u0027) as f:\\n\",\n    \"        data \u003d next(itertools.islice(csv.reader(f), line, None))\\n\",\n    \"    return data\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"code_folding\": [\n     5,\n     42,\n     57,\n     64,\n     68,\n     72,\n     78,\n     126,\n     158,\n     195,\n     258,\n     267,\n     276,\n     285,\n     294\n    ]\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#####################################################################################################################\\n\",\n    \"#  Classifiers: Code directly modified based on the PyTorch Model Zoo Implementations\\n\",\n    \"#####################################################################################################################\\n\",\n    \"\\n\",\n    \"## 1D variant of VGG model take 200 dimensional fixed time series inputs\\n\",\n    \"class VGG(nn.Module):\\n\",\n    \"\\n\",\n    \"    def __init__(self, features, num_classes, arch\u003d\u0027vgg\u0027):\\n\",\n    \"        super(VGG, self).__init__()\\n\",\n    \"        self.arch \u003d arch\\n\",\n    \"        self.features \u003d features\\n\",\n    \"        self.classifier \u003d nn.Sequential(\\n\",\n    \"            nn.Linear(512 * 6, 512),\\n\",\n    \"            nn.ReLU(True),\\n\",\n    \"            nn.Dropout(),\\n\",\n    \"            nn.Linear(512, 512),\\n\",\n    \"            nn.ReLU(True),\\n\",\n    \"            nn.Dropout(),\\n\",\n    \"            nn.Linear(512, num_classes),\\n\",\n    \"        )\\n\",\n    \"        self._initialize_weights()\\n\",\n    \"\\n\",\n    \"    def forward(self, x):\\n\",\n    \"        x \u003d self.features(x)\\n\",\n    \"        x \u003d x.view(x.size(0), -1)\\n\",\n    \"        x \u003d self.classifier(x)\\n\",\n    \"        return x\\n\",\n    \"\\n\",\n    \"    def _initialize_weights(self):\\n\",\n    \"        for m in self.modules():\\n\",\n    \"            if isinstance(m, nn.Conv1d):\\n\",\n    \"                n \u003d m.kernel_size[0] * m.out_channels\\n\",\n    \"                m.weight.data.normal_(0, math.sqrt(2. / n))\\n\",\n    \"                if m.bias is not None:\\n\",\n    \"                    m.bias.data.zero_()\\n\",\n    \"            elif isinstance(m, nn.BatchNorm1d):\\n\",\n    \"                m.weight.data.fill_(1)\\n\",\n    \"                m.bias.data.zero_()\\n\",\n    \"            elif isinstance(m, nn.Linear):\\n\",\n    \"                m.weight.data.normal_(0, 0.01)\\n\",\n    \"                m.bias.data.zero_()\\n\",\n    \"\\n\",\n    \"def make_layers(cfg, batch_norm\u003dFalse):\\n\",\n    \"    layers \u003d []\\n\",\n    \"    in_channels \u003d 1\\n\",\n    \"    for v in cfg:\\n\",\n    \"        if v \u003d\u003d \u0027M\u0027:\\n\",\n    \"            layers +\u003d [nn.MaxPool1d(kernel_size\u003d2, stride\u003d2)]\\n\",\n    \"        else:\\n\",\n    \"            conv1d \u003d nn.Conv1d(in_channels, v, kernel_size\u003d3, padding\u003d1)\\n\",\n    \"            if batch_norm:\\n\",\n    \"                layers +\u003d [conv1d, nn.BatchNorm1d(v), nn.ReLU(inplace\u003dTrue)]\\n\",\n    \"            else:\\n\",\n    \"                layers +\u003d [conv1d, nn.ReLU(inplace\u003dTrue)]\\n\",\n    \"            in_channels \u003d v\\n\",\n    \"    return nn.Sequential(*layers)\\n\",\n    \"\\n\",\n    \"cfg \u003d {\\n\",\n    \"    \u0027A\u0027: [64, \u0027M\u0027, 128, \u0027M\u0027, 256, 256, \u0027M\u0027, 512, 512, \u0027M\u0027, 512, 512, \u0027M\u0027],\\n\",\n    \"    \u0027B\u0027: [64, 64, \u0027M\u0027, 128, 128, \u0027M\u0027, 256, 256, \u0027M\u0027, 512, 512, \u0027M\u0027, 512, 512, \u0027M\u0027],\\n\",\n    \"    \u0027D\u0027: [64, 64, \u0027M\u0027, 128, 128, \u0027M\u0027, 256, 256, 256, \u0027M\u0027, 512, 512, 512, \u0027M\u0027, 512, 512, 512, \u0027M\u0027],\\n\",\n    \"    \u0027E\u0027: [64, 64, \u0027M\u0027, 128, 128, \u0027M\u0027, 256, 256, 256, 256, \u0027M\u0027, 512, 512, 512, 512, \u0027M\u0027, 512, 512, 512, 512, \u0027M\u0027],\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"def vgg13bn(**kwargs):\\n\",\n    \"    model \u003d VGG(make_layers(cfg[\u0027B\u0027], batch_norm\u003dTrue), arch\u003d\u0027vgg13bn\u0027, **kwargs)\\n\",\n    \"    return model\\n\",\n    \"\\n\",\n    \"def vgg16bn(**kwargs):\\n\",\n    \"    model \u003d VGG(make_layers(cfg[\u0027D\u0027], batch_norm\u003dTrue), arch\u003d\u0027vgg16bn\u0027, **kwargs)\\n\",\n    \"    return model\\n\",\n    \"\\n\",\n    \"def vgg19bn(**kwargs):\\n\",\n    \"    model \u003d VGG(make_layers(cfg[\u0027E\u0027], batch_norm\u003dTrue), arch\u003d\u0027vgg19bn\u0027, **kwargs)\\n\",\n    \"    return model\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"## Multilayer LSTM based classifier taking in 200 dimensional fixed time series inputs\\n\",\n    \"class LSTMClassifier(nn.Module):\\n\",\n    \"\\n\",\n    \"    def __init__(self, in_dim, hidden_dim, num_layers, dropout, bidirectional, num_classes, batch_size):\\n\",\n    \"        super(LSTMClassifier, self).__init__()\\n\",\n    \"        self.arch \u003d \u0027lstm\u0027\\n\",\n    \"        self.hidden_dim \u003d hidden_dim\\n\",\n    \"        self.batch_size \u003d batch_size\\n\",\n    \"        self.num_dir \u003d 2 if bidirectional else 1\\n\",\n    \"        self.num_layers \u003d num_layers\\n\",\n    \"\\n\",\n    \"        self.lstm \u003d nn.LSTM(\\n\",\n    \"                input_size\u003din_dim,\\n\",\n    \"                hidden_size\u003dhidden_dim,\\n\",\n    \"                num_layers\u003dnum_layers,\\n\",\n    \"                dropout\u003ddropout,\\n\",\n    \"                bidirectional\u003dbidirectional\\n\",\n    \"            )\\n\",\n    \"\\n\",\n    \"        self.hidden2label \u003d nn.Sequential(\\n\",\n    \"            nn.Linear(hidden_dim*self.num_dir, hidden_dim),\\n\",\n    \"            nn.ReLU(True),\\n\",\n    \"            nn.Dropout(),\\n\",\n    \"            nn.Linear(hidden_dim, hidden_dim),\\n\",\n    \"            nn.ReLU(True),\\n\",\n    \"            nn.Dropout(),\\n\",\n    \"            nn.Linear(hidden_dim, num_classes),\\n\",\n    \"        )\\n\",\n    \"\\n\",\n    \"        # self.hidden \u003d self.init_hidden()\\n\",\n    \"\\n\",\n    \"    def init_hidden(self):\\n\",\n    \"        if cuda:\\n\",\n    \"            h0 \u003d Variable(torch.zeros(self.num_layers*self.num_dir, self.batch_size, self.hidden_dim).cuda())\\n\",\n    \"            c0 \u003d Variable(torch.zeros(self.num_layers*self.num_dir, self.batch_size, self.hidden_dim).cuda())\\n\",\n    \"        else:\\n\",\n    \"            h0 \u003d Variable(torch.zeros(self.num_layers*self.num_dir, self.batch_size, self.hidden_dim))\\n\",\n    \"            c0 \u003d Variable(torch.zeros(self.num_layers*self.num_dir, self.batch_size, self.hidden_dim))\\n\",\n    \"        return (h0, c0)\\n\",\n    \"\\n\",\n    \"    def forward(self, x): # x is (batch_size, 1, 200), permute to (200, batch_size, 1)\\n\",\n    \"        x \u003d x.permute(2, 0, 1)\\n\",\n    \"        # See: https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426/2\\n\",\n    \"        lstm_out, (h, c) \u003d self.lstm(x, self.init_hidden())\\n\",\n    \"        y  \u003d self.hidden2label(lstm_out[-1])\\n\",\n    \"        return y\\n\",\n    \"    \\n\",\n    \"\\n\",\n    \"## 1D Variant of ResNet taking in 200 dimensional fixed time series inputs\\n\",\n    \"class BasicBlock(nn.Module):\\n\",\n    \"    expansion \u003d 1\\n\",\n    \"\\n\",\n    \"    def __init__(self, inplanes, planes, stride\u003d1, downsample\u003dNone):\\n\",\n    \"        super(BasicBlock, self).__init__()\\n\",\n    \"        self.conv1 \u003d nn.Conv1d(inplanes, planes, kernel_size\u003d3, padding\u003d1, stride\u003dstride, bias\u003dFalse)\\n\",\n    \"        self.bn1 \u003d nn.BatchNorm1d(planes)\\n\",\n    \"        self.relu \u003d nn.ReLU(inplace\u003dTrue)\\n\",\n    \"        self.conv2 \u003d nn.Conv1d(planes, planes, kernel_size\u003d3, padding\u003d1, stride\u003dstride, bias\u003dFalse)\\n\",\n    \"        self.bn2 \u003d nn.BatchNorm1d(planes)\\n\",\n    \"        self.downsample \u003d downsample\\n\",\n    \"\\n\",\n    \"        self.stride \u003d stride\\n\",\n    \"\\n\",\n    \"    def forward(self, x):\\n\",\n    \"        residual \u003d x\\n\",\n    \"\\n\",\n    \"        out \u003d self.conv1(x)\\n\",\n    \"        out \u003d self.bn1(out)\\n\",\n    \"        out \u003d self.relu(out)\\n\",\n    \"\\n\",\n    \"        out \u003d self.conv2(out)\\n\",\n    \"        out \u003d self.bn2(out)\\n\",\n    \"\\n\",\n    \"        if self.downsample is not None:\\n\",\n    \"            residual \u003d self.downsample(x)\\n\",\n    \"        # print(\u0027out\u0027, out.size(), \u0027res\u0027, residual.size(), self.downsample)\\n\",\n    \"        out +\u003d residual\\n\",\n    \"        out \u003d self.relu(out)\\n\",\n    \"\\n\",\n    \"        return out\\n\",\n    \"\\n\",\n    \"class Bottleneck(nn.Module):\\n\",\n    \"    expansion \u003d 4\\n\",\n    \"\\n\",\n    \"    def __init__(self, inplanes, planes, stride\u003d1, downsample\u003dNone):\\n\",\n    \"        super(Bottleneck, self).__init__()\\n\",\n    \"        self.conv1 \u003d nn.Conv1d(inplanes, planes, kernel_size\u003d1, padding\u003d1, stride\u003dstride, bias\u003dFalse)\\n\",\n    \"        self.bn1 \u003d nn.BatchNorm1d(planes)\\n\",\n    \"        self.conv2 \u003d nn.Conv1d(planes, planes, kernel_size\u003d1, padding\u003d1, stride\u003dstride, bias\u003dFalse)\\n\",\n    \"        self.bn2 \u003d nn.BatchNorm1d(planes)\\n\",\n    \"        self.conv3 \u003d nn.Conv1d(planes, planes * 4, kernel_size\u003d1, padding\u003d1, stride\u003dstride, bias\u003dFalse)\\n\",\n    \"        self.bn3 \u003d nn.BatchNorm1d(planes * 4)\\n\",\n    \"        self.relu \u003d nn.ReLU(inplace\u003dTrue)\\n\",\n    \"        self.downsample \u003d downsample\\n\",\n    \"        self.stride \u003d stride\\n\",\n    \"\\n\",\n    \"    def forward(self, x):\\n\",\n    \"        residual \u003d x\\n\",\n    \"\\n\",\n    \"        out \u003d self.conv1(x)\\n\",\n    \"        out \u003d self.bn1(out)\\n\",\n    \"        out \u003d self.relu(out)\\n\",\n    \"\\n\",\n    \"        out \u003d self.conv2(out)\\n\",\n    \"        out \u003d self.bn2(out)\\n\",\n    \"        out \u003d self.relu(out)\\n\",\n    \"\\n\",\n    \"        out \u003d self.conv3(out)\\n\",\n    \"        out \u003d self.bn3(out)\\n\",\n    \"\\n\",\n    \"        if self.downsample is not None:\\n\",\n    \"            residual \u003d self.downsample(x)\\n\",\n    \"\\n\",\n    \"        out +\u003d residual\\n\",\n    \"        out \u003d self.relu(out)\\n\",\n    \"\\n\",\n    \"        return out\\n\",\n    \"\\n\",\n    \"class ResNet(nn.Module):\\n\",\n    \"\\n\",\n    \"    def __init__(self, block, layers, num_classes, arch):\\n\",\n    \"        self.inplanes \u003d 64\\n\",\n    \"        super(ResNet, self).__init__()\\n\",\n    \"        self.conv1 \u003d nn.Conv1d(1, 64, kernel_size\u003d7, stride\u003d2, padding\u003d3,\\n\",\n    \"                               bias\u003dFalse)\\n\",\n    \"        self.bn1 \u003d nn.BatchNorm1d(64)\\n\",\n    \"        self.relu \u003d nn.ReLU(inplace\u003dTrue)\\n\",\n    \"        self.maxpool \u003d nn.MaxPool1d(kernel_size\u003d3, stride\u003d2, padding\u003d1)\\n\",\n    \"        self.layer1 \u003d self._make_layer(block, 64, layers[0])\\n\",\n    \"        self.layer2 \u003d self._make_layer(block, 128, layers[1]) #, stride\u003d2)\\n\",\n    \"        self.layer3 \u003d self._make_layer(block, 256, layers[2]) #, stride\u003d2)\\n\",\n    \"        self.layer4 \u003d self._make_layer(block, 512, layers[3]) #, stride\u003d2)\\n\",\n    \"        self.avgpool \u003d nn.AvgPool1d(7, stride\u003d1)\\n\",\n    \"        self.fc \u003d nn.Linear(22528 , num_classes) # 512 * block.expansion\\n\",\n    \"        \\n\",\n    \"        self.arch \u003d arch\\n\",\n    \"\\n\",\n    \"        for m in self.modules():\\n\",\n    \"            if isinstance(m, nn.Conv1d):\\n\",\n    \"                n \u003d m.kernel_size[0] * m.out_channels\\n\",\n    \"                m.weight.data.normal_(0, math.sqrt(2. / n))\\n\",\n    \"            elif isinstance(m, nn.BatchNorm1d):\\n\",\n    \"                m.weight.data.fill_(1)\\n\",\n    \"                m.bias.data.zero_()\\n\",\n    \"\\n\",\n    \"    def _make_layer(self, block, planes, blocks, stride\u003d1):\\n\",\n    \"        downsample \u003d None\\n\",\n    \"        if stride !\u003d 1 or self.inplanes !\u003d planes * block.expansion:\\n\",\n    \"            downsample \u003d nn.Sequential(\\n\",\n    \"                nn.Conv1d(self.inplanes, planes * block.expansion,\\n\",\n    \"                          kernel_size\u003d1, stride\u003dstride, bias\u003dFalse),\\n\",\n    \"                nn.BatchNorm1d(planes * block.expansion),\\n\",\n    \"            )\\n\",\n    \"\\n\",\n    \"        layers \u003d []\\n\",\n    \"        layers.append(block(self.inplanes, planes, stride, downsample))\\n\",\n    \"        self.inplanes \u003d planes * block.expansion\\n\",\n    \"        for i in range(1, blocks):\\n\",\n    \"            layers.append(block(self.inplanes, planes))\\n\",\n    \"\\n\",\n    \"        return nn.Sequential(*layers)\\n\",\n    \"\\n\",\n    \"    def forward(self, x):\\n\",\n    \"        x \u003d self.conv1(x)\\n\",\n    \"        x \u003d self.bn1(x)\\n\",\n    \"        x \u003d self.relu(x)\\n\",\n    \"        x \u003d self.maxpool(x)\\n\",\n    \"\\n\",\n    \"        x \u003d self.layer1(x)\\n\",\n    \"        \\n\",\n    \"        x \u003d self.layer2(x)\\n\",\n    \"        x \u003d self.layer3(x)\\n\",\n    \"        x \u003d self.layer4(x)\\n\",\n    \"\\n\",\n    \"        x \u003d self.avgpool(x)\\n\",\n    \"        x \u003d x.view(x.size(0), -1)\\n\",\n    \"        # print(x.size())\\n\",\n    \"        x \u003d self.fc(x)\\n\",\n    \"\\n\",\n    \"        return x\\n\",\n    \"\\n\",\n    \"def resnet18(pretrained\u003dFalse, **kwargs):\\n\",\n    \"    \\\"\\\"\\\"Constructs a ResNet-18 model.\\n\",\n    \"    Args:\\n\",\n    \"        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    model \u003d ResNet(BasicBlock, [2, 2, 2, 2], arch\u003d\u0027resnet18\u0027, **kwargs)\\n\",\n    \"\\n\",\n    \"    return model\\n\",\n    \"\\n\",\n    \"def resnet34(pretrained\u003dFalse, **kwargs):\\n\",\n    \"    \\\"\\\"\\\"Constructs a ResNet-34 model.\\n\",\n    \"    Args:\\n\",\n    \"        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    model \u003d ResNet(BasicBlock, [3, 4, 6, 3], arch\u003d\u0027resnet34\u0027, **kwargs)\\n\",\n    \"\\n\",\n    \"    return model\\n\",\n    \"\\n\",\n    \"def resnet50(pretrained\u003dFalse, **kwargs):\\n\",\n    \"    \\\"\\\"\\\"Constructs a ResNet-50 model.\\n\",\n    \"    Args:\\n\",\n    \"        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    model \u003d ResNet(Bottleneck, [3, 4, 6, 3], arch\u003d\u0027resnet50\u0027, **kwargs)\\n\",\n    \"\\n\",\n    \"    return model\\n\",\n    \"\\n\",\n    \"def resnet101(pretrained\u003dFalse, **kwargs):\\n\",\n    \"    \\\"\\\"\\\"Constructs a ResNet-101 model.\\n\",\n    \"    Args:\\n\",\n    \"        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    model \u003d ResNet(Bottleneck, [3, 4, 23, 3], arch\u003d\u0027resnet101\u0027, **kwargs)\\n\",\n    \"\\n\",\n    \"    return model\\n\",\n    \"\\n\",\n    \"def resnet152(pretrained\u003dFalse, **kwargs):\\n\",\n    \"    \\\"\\\"\\\"Constructs a ResNet-152 model.\\n\",\n    \"    Args:\\n\",\n    \"        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    model \u003d ResNet(Bottleneck, [3, 8, 36, 3], arch\u003d\u0027resnet152\u0027, **kwargs)\\n\",\n    \"    \\n\",\n    \"    return model\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"code_folding\": [\n     5\n    ]\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#####################################################################################################################\\n\",\n    \"#  DataLoaders\\n\",\n    \"#####################################################################################################################\\n\",\n    \"\\n\",\n    \"# Dummy Dataset for testing purpose only\\n\",\n    \"class DummyDataset(Dataset):\\n\",\n    \"    \\\"\\\"\\\"Time Series dataset.\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"    def __init__(self, numclasses\u003d15):\\n\",\n    \"        self.numclasses \u003d numclasses\\n\",\n    \"\\n\",\n    \"    def __len__(self):\\n\",\n    \"        return 512\\n\",\n    \"\\n\",\n    \"    def __getitem__(self, idx):\\n\",\n    \"        data \u003d np.random.randn(200)\\n\",\n    \"        data \u003d np.expand_dims(data, axis\u003d0)\\n\",\n    \"\\n\",\n    \"        return data, np.random.randint(self.numclasses)\\n\",\n    \"    \\n\",\n    \"# Synchronized Time Series, allow variable training window\\n\",\n    \"class TimeSeriesDataset(Dataset):\\n\",\n    \"    \\\"\\\"\\\"Time Series dataset.\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"    def __init__(self, data_path, norm, is_train, train_window):\\n\",\n    \"\\n\",\n    \"        self.file_dirs \u003d sorted([f for f in glob.glob(\u0027%s/*/*/*\u0027%data_path) if os.path.isdir(f)])\\n\",\n    \"        self.norm \u003d norm\\n\",\n    \"        self.is_train \u003d is_train\\n\",\n    \"        self.train_window \u003d train_window\\n\",\n    \"        self.files_per_dir \u003d self.train_window**2 if self.is_train else (32**2 - self.train_window**2)\\n\",\n    \"\\n\",\n    \"    def __len__(self):\\n\",\n    \"        return int( len(self.file_dirs) * self.files_per_dir )\\n\",\n    \"\\n\",\n    \"    def __getitem__(self, idx):\\n\",\n    \"        fdir \u003d self.file_dirs[ idx // self.files_per_dir ] \\n\",\n    \"        file_id \u003d idx % self.files_per_dir + ( 0 if self.is_train else (self.train_window**2) )\\n\",\n    \"        data \u003d np.fromfile(os.path.join(fdir, \u0027%s.bin\u0027%str(file_id).zfill(4)))\\n\",\n    \"        target \u003d int(fdir.split(\u0027/\u0027)[3]) # Be careful about this index !!\\n\",\n    \"        \\n\",\n    \"        if self.norm:\\n\",\n    \"            data -\u003d data.mean()\\n\",\n    \"            data /\u003d data.std()\\n\",\n    \"        else:\\n\",\n    \"            data -\u003d data[0]\\n\",\n    \"        \\n\",\n    \"        data \u003d np.expand_dims(data, axis\u003d0)\\n\",\n    \"        return data, target\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"code_folding\": [\n     138\n    ]\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#####################################################################################################################\\n\",\n    \"#  Trainer and Core Experiment Scripts\\n\",\n    \"#####################################################################################################################\\n\",\n    \"\\n\",\n    \"def run_trainer(experiment_path, model_path, model, train_loader, test_loader, get_acc, resume, batch_size, num_epoch):\\n\",\n    \"\\n\",\n    \"    if not os.path.exists(model_path):\\n\",\n    \"        os.makedirs(model_path)\\n\",\n    \"    def save_checkpoint(state, is_best, filename\u003dmodel_path+\u0027checkpoint.pth.tar\u0027):\\n\",\n    \"        torch.save(state, filename)\\n\",\n    \"        if is_best:\\n\",\n    \"            shutil.copyfile(filename, model_path+\u0027model_best.pth.tar\u0027)\\n\",\n    \"    def get_last_checkpoint(model_path):\\n\",\n    \"        fs \u003d sorted([f for f in os.listdir(model_path) if \u0027Epoch\u0027 in f], key\u003dlambda k: int(k.split()[1]))\\n\",\n    \"        return model_path+fs[-1] if len(fs) \u003e 0 else None\\n\",\n    \"    \\n\",\n    \"    start_epoch \u003d 0\\n\",\n    \"    best_res \u003d 0\\n\",\n    \"    lrcurve \u003d []\\n\",\n    \"    conf_mats \u003d []\\n\",\n    \"    resume_state \u003d get_last_checkpoint(model_path) if resume else None\\n\",\n    \"    if resume_state and os.path.isfile(resume_state):\\n\",\n    \"        print(\\\"\u003d\u003e loading checkpoint \u0027{}\u0027\\\".format(resume_state))\\n\",\n    \"        checkpoint \u003d torch.load(resume_state)\\n\",\n    \"        start_epoch \u003d checkpoint[\u0027epoch\u0027]+1\\n\",\n    \"        best_res \u003d checkpoint[\u0027val_acc\u0027]\\n\",\n    \"        lrcurve \u003d checkpoint[\u0027lrcurve\u0027]\\n\",\n    \"        conf_mats \u003d checkpoint[\u0027conf_mats\u0027]\\n\",\n    \"        model.load_state_dict(checkpoint[\u0027state_dict\u0027])\\n\",\n    \"        if cuda:\\n\",\n    \"            model.cuda()\\n\",\n    \"        optimizer \u003d optim.Adam(model.parameters())\\n\",\n    \"        optimizer.load_state_dict(checkpoint[\u0027optimizer\u0027])\\n\",\n    \"        print(\\\"\u003d\u003e loaded checkpoint \u0027{}\u0027 (epoch {})\\\"\\n\",\n    \"              .format(resume_state, checkpoint[\u0027epoch\u0027]))\\n\",\n    \"    else:\\n\",\n    \"        if cuda:\\n\",\n    \"            model.cuda()\\n\",\n    \"        optimizer \u003d optim.Adam(model.parameters())\\n\",\n    \"\\n\",\n    \"    criterion \u003d nn.CrossEntropyLoss()\\n\",\n    \"    # scheduler \u003d ReduceLROnPlateau(optimizer, \u0027min\u0027, factor\u003d0.5) # optim.lr_scheduler.ReduceLROnPlateau(optimizer, \u0027min\u0027, factor\u003d0.5)\\n\",\n    \"\\n\",\n    \"    def train(epoch):\\n\",\n    \"        model.train()\\n\",\n    \"        total, total_correct \u003d 0., 0.\\n\",\n    \"        train_conf_mats \u003d []\\n\",\n    \"        for batch_idx, (data, target) in enumerate(train_loader):\\n\",\n    \"            data, target \u003d Variable(data.float()), Variable(target.long())\\n\",\n    \"            if cuda:\\n\",\n    \"                data, target \u003d data.cuda(), target.cuda()\\n\",\n    \"            optimizer.zero_grad()\\n\",\n    \"            output \u003d model(data)\\n\",\n    \"            loss \u003d criterion(output, target)\\n\",\n    \"            loss.backward()\\n\",\n    \"            optimizer.step()\\n\",\n    \"\\n\",\n    \"            _, correct, num_instance, conf_mat \u003d get_acc(output, target)\\n\",\n    \"            total_correct +\u003d correct\\n\",\n    \"            total +\u003d num_instance\\n\",\n    \"            train_conf_mats.append(conf_mat)\\n\",\n    \"            if batch_idx % 10 \u003d\u003d 0:\\n\",\n    \"                print(\u0027Train Epoch: {} [{}/{} ({:.0f}%)]\\\\tLoss: {:.6f} Acc: {:.2f}%/{:.2f}%\u0027.format(\\n\",\n    \"                    epoch, batch_idx * len(data), len(train_loader.dataset),\\n\",\n    \"                    100. * batch_idx / len(train_loader), loss.data[0],\\n\",\n    \"                    100. * correct / num_instance, 100. * total_correct / total ))\\n\",\n    \"        \\n\",\n    \"        return 100. * total_correct / total, np.dstack(train_conf_mats).sum(axis\u003d2)\\n\",\n    \"\\n\",\n    \"    def test():\\n\",\n    \"        model.eval()\\n\",\n    \"        test_loss \u003d 0.\\n\",\n    \"        total, total_correct \u003d 0., 0.\\n\",\n    \"        test_conf_mats \u003d []\\n\",\n    \"        preds \u003d []\\n\",\n    \"        for data, target in test_loader:\\n\",\n    \"            data, target \u003d Variable(data.float(), volatile\u003dTrue), Variable(target.long())\\n\",\n    \"            if cuda:\\n\",\n    \"                data, target \u003d data.cuda(), target.cuda()\\n\",\n    \"            output \u003d model(data)\\n\",\n    \"            test_loss +\u003d criterion(output, target).data[0] # sum up batch loss\\n\",\n    \"            \\n\",\n    \"            pred, correct, num_instance, conf_mat \u003d get_acc(output, target)\\n\",\n    \"            total_correct +\u003d correct\\n\",\n    \"            total +\u003d num_instance\\n\",\n    \"            test_conf_mats.append(conf_mat)\\n\",\n    \"            preds.append(pred)\\n\",\n    \"\\n\",\n    \"        test_acc \u003d 100. * total_correct / total\\n\",\n    \"        test_loss /\u003d len(test_loader.dataset)\\n\",\n    \"        print(\u0027\\\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\\\n\u0027.format(\\n\",\n    \"            test_loss, total_correct, total,\\n\",\n    \"            test_acc))\\n\",\n    \"\\n\",\n    \"        return np.vstack(preds), test_acc, np.dstack(test_conf_mats).sum(axis\u003d2)\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"    for epoch in range(start_epoch, num_epoch):\\n\",\n    \"        is_best \u003d False\\n\",\n    \"\\n\",\n    \"        train_acc, train_conf \u003d train(epoch)\\n\",\n    \"        preds, val_acc, val_conf \u003d test()\\n\",\n    \"        \\n\",\n    \"        print(\\\"Training Confmat: \\\")\\n\",\n    \"        print(train_conf)\\n\",\n    \"        print(\\\"Testing Confmat: \\\")\\n\",\n    \"        print(val_conf)\\n\",\n    \"        print(\\\"Number of Predictions Made: \\\")\\n\",\n    \"        print(preds.shape)\\n\",\n    \"        \\n\",\n    \"        lrcurve.append((train_acc, val_acc))\\n\",\n    \"        conf_mats.append((train_conf, val_conf))\\n\",\n    \"        # scheduler.step(val_loss)\\n\",\n    \"\\n\",\n    \"        if val_acc \u003e best_res:\\n\",\n    \"            best_res \u003d val_acc\\n\",\n    \"            is_best \u003d True\\n\",\n    \"\\n\",\n    \"        save_checkpoint({\\n\",\n    \"                \u0027epoch\u0027: epoch,\\n\",\n    \"                \u0027arch\u0027: model.arch,\\n\",\n    \"                \u0027state_dict\u0027: model.cpu().state_dict(),\\n\",\n    \"                \u0027train_acc\u0027:train_acc,\\n\",\n    \"                \u0027val_acc\u0027: val_acc,\\n\",\n    \"                \u0027optimizer\u0027 : optimizer.state_dict(),\\n\",\n    \"                \u0027lrcurve\u0027:lrcurve,\\n\",\n    \"                \u0027train_conf\u0027:train_conf,\\n\",\n    \"                \u0027val_conf\u0027:val_conf,\\n\",\n    \"                \u0027conf_mats\u0027:conf_mats,\\n\",\n    \"                \u0027test_predictions\u0027:preds,\\n\",\n    \"            }, is_best,\\n\",\n    \"            model_path+\\\"Epoch %d Acc %.4f.pt\\\"%(epoch, val_acc))\\n\",\n    \"\\n\",\n    \"        if cuda:\\n\",\n    \"            model.cuda()\\n\",\n    \"            \\n\",\n    \"    return lrcurve, conf_mats\\n\",\n    \"\\n\",\n    \"def run_dummy_experiment(experiment_path, model_root, models, norm, train_window, get_acc, resume\u003dFalse, num_epoch\u003d10):\\n\",\n    \"    \\n\",\n    \"    exp_result \u003d {}\\n\",\n    \"    for batch_size, model in models:\\n\",\n    \"        print(\\\"Running %s\\\" % model.arch)\\n\",\n    \"        \\n\",\n    \"        print(\u0027Loading Data..\u0027)\\n\",\n    \"        train_data \u003d DummyDataset()\\n\",\n    \"        test_data \u003d DummyDataset()\\n\",\n    \"        train_loader \u003d DataLoader(train_data, batch_size\u003dbatch_size, shuffle\u003dTrue, num_workers\u003d16)\\n\",\n    \"        test_loader \u003d DataLoader(test_data, batch_size\u003dbatch_size, shuffle\u003dTrue, num_workers\u003d16)\\n\",\n    \"    \\n\",\n    \"        model_path \u003d os.path.join(\\n\",\n    \"            experiment_path, \\n\",\n    \"            model_root,\\n\",\n    \"            \u0027norm\u0027 if norm else \u0027nonorm\u0027,\\n\",\n    \"            str(train_window),\\n\",\n    \"            model.arch) + \u0027/\u0027\\n\",\n    \"        lrcurve, conf_mats \u003d run_trainer(experiment_path, model_path, model, train_loader, test_loader, get_acc, resume, batch_size, num_epoch)\\n\",\n    \"        exp_result[model.arch] \u003d {\u0027lrcurve\u0027:lrcurve, \u0027conf_mats\u0027:conf_mats}\\n\",\n    \"        \\n\",\n    \"    return exp_result\\n\",\n    \"        \\n\",\n    \"def run_experiment(experiment_path, data_path, model_root, models, norm, train_window, get_acc, resume\u003dFalse, num_epoch\u003d10):\\n\",\n    \"    \\n\",\n    \"    exp_result \u003d {}\\n\",\n    \"    for batch_size, model in models:\\n\",\n    \"        print(\\\"Running %s\\\" % model.arch)\\n\",\n    \"        \\n\",\n    \"        print(\u0027Loading Data..\u0027)\\n\",\n    \"        train_data \u003d TimeSeriesDataset(data_path, norm, True, train_window)\\n\",\n    \"        test_data \u003d TimeSeriesDataset(data_path, norm, False, train_window)\\n\",\n    \"        train_loader \u003d DataLoader(train_data, batch_size\u003dbatch_size, shuffle\u003dTrue, num_workers\u003d0)\\n\",\n    \"        test_loader \u003d DataLoader(test_data, batch_size\u003dbatch_size, shuffle\u003dFalse, num_workers\u003d0)\\n\",\n    \"    \\n\",\n    \"        model_path \u003d os.path.join(\\n\",\n    \"            experiment_path, \\n\",\n    \"            model_root,\\n\",\n    \"            \u0027norm\u0027 if norm else \u0027nonorm\u0027,\\n\",\n    \"            str(train_window),\\n\",\n    \"            model.arch) + \u0027/\u0027\\n\",\n    \"        lrcurve, conf_mats \u003d run_trainer(experiment_path, model_path, model, train_loader, test_loader, get_acc, resume, batch_size, num_epoch)\\n\",\n    \"        exp_result[model.arch] \u003d {\u0027lrcurve\u0027:lrcurve, \u0027conf_mats\u0027:conf_mats}\\n\",\n    \"        \\n\",\n    \"    return exp_result\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"code_folding\": [\n     19\n    ]\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#####################################################################################################################\\n\",\n    \"#  Dummy Experiment for Testing Purpose\\n\",\n    \"#####################################################################################################################\\n\",\n    \"\\n\",\n    \"def get_models():\\n\",\n    \"    return [\\n\",\n    \"        (1024, vgg13bn(num_classes\u003d15)),\\n\",\n    \"        (256, LSTMClassifier(\\n\",\n    \"            in_dim\u003d1,\\n\",\n    \"            hidden_dim\u003d120,\\n\",\n    \"            num_layers\u003d3,\\n\",\n    \"            dropout\u003d0.8,\\n\",\n    \"            bidirectional\u003dTrue,\\n\",\n    \"            num_classes\u003d15,\\n\",\n    \"            batch_size\u003d256\\n\",\n    \"        )),\\n\",\n    \"        (1024, resnet34(num_classes\u003d15))\\n\",\n    \"    ]\\n\",\n    \"\\n\",\n    \"def get_acc(output, target):\\n\",\n    \"    # takes in two tensors to compute accuracy\\n\",\n    \"    pred \u003d output.data.max(1, keepdim\u003dTrue)[1] # get the index of the max log-probability\\n\",\n    \"    correct \u003d pred.eq(target.data.view_as(pred)).cpu().sum()\\n\",\n    \"    conf_mat \u003d confusion_matrix(pred.cpu().numpy(), target.data.cpu().numpy(), labels\u003drange(15))\\n\",\n    \"    return pred.cpu().numpy(), correct, target.size(0), conf_mat\\n\",\n    \"\\n\",\n    \"dummy_experiment \u003d {\\n\",\n    \"    \u0027experiment_path\u0027:\u0027dummy\u0027, \\n\",\n    \"    \u0027model_root\u0027:\u0027model\u0027, \\n\",\n    \"    \u0027models\u0027:get_models(), \\n\",\n    \"    \u0027norm\u0027:False, \\n\",\n    \"    \u0027train_window\u0027:24, \\n\",\n    \"    \u0027get_acc\u0027: get_acc,\\n\",\n    \"    \u0027resume\u0027:False, \\n\",\n    \"    \u0027num_epoch\u0027:3\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"# exp_log \u003d run_dummy_experiment(**experiment)\\n\",\n    \"# print(exp_log)\\n\",\n    \"# experiment_filepath \u003d os.path.join(\\n\",\n    \"#                         experiment[\u0027experiment_path\u0027], \\n\",\n    \"#                         experiment[\u0027model_root\u0027],\\n\",\n    \"#                         \u0027norm\u0027 if experiment[\u0027norm\u0027] else \u0027nonorm\u0027,\\n\",\n    \"#                         str(experiment[\u0027train_window\u0027]),\\n\",\n    \"#                         \u0027exp_log.pkl\u0027)\\n\",\n    \"# save_pickle(exp_log, experiment_filepath)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#####################################################################################################################\\n\",\n    \"#  Material Recognition Experiment with Deep Models\\n\",\n    \"#####################################################################################################################\\n\",\n    \"\\n\",\n    \"def get_models(): # tuples of (batch_size, model)\\n\",\n    \"    return [\\n\",\n    \"        (1024, vgg13bn(num_classes\u003d15)),\\n\",\n    \"        (256, LSTMClassifier(\\n\",\n    \"            in_dim\u003d1,\\n\",\n    \"            hidden_dim\u003d120,\\n\",\n    \"            num_layers\u003d3,\\n\",\n    \"            dropout\u003d0.8,\\n\",\n    \"            bidirectional\u003dTrue,\\n\",\n    \"            num_classes\u003d15,\\n\",\n    \"            batch_size\u003d256\\n\",\n    \"        )),\\n\",\n    \"        (1024, resnet34(num_classes\u003d15))\\n\",\n    \"    ]\\n\",\n    \"\\n\",\n    \"def get_acc(output, target):\\n\",\n    \"    # takes in two tensors to compute accuracy\\n\",\n    \"    pred \u003d output.data.max(1, keepdim\u003dTrue)[1] # get the index of the max log-probability\\n\",\n    \"    correct \u003d pred.eq(target.data.view_as(pred)).cpu().sum()\\n\",\n    \"    conf_mat \u003d confusion_matrix(pred.cpu().numpy(), target.data.cpu().numpy(), labels\u003drange(15))\\n\",\n    \"    return np.squeeze(pred.cpu().numpy()), correct, target.size(0), conf_mat\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"experiments \u003d [\\n\",\n    \"    {\\n\",\n    \"        \u0027experiment_path\u0027:\u0027roll\u0027, \\n\",\n    \"        \u0027data_path\u0027:\u0027roll/roll\u0027,\\n\",\n    \"        \u0027model_root\u0027:\u0027model\u0027, \\n\",\n    \"        \u0027models\u0027:get_models(),\\n\",\n    \"        \u0027norm\u0027:False, \\n\",\n    \"        \u0027train_window\u0027:24, \\n\",\n    \"        \u0027get_acc\u0027: get_acc,\\n\",\n    \"        \u0027resume\u0027:True,  \\n\",\n    \"        \u0027num_epoch\u0027:10\\n\",\n    \"    },\\n\",\n    \"    \\n\",\n    \"    {\\n\",\n    \"        \u0027experiment_path\u0027:\u0027roll\u0027, \\n\",\n    \"        \u0027data_path\u0027:\u0027roll/roll\u0027,\\n\",\n    \"        \u0027model_root\u0027:\u0027model\u0027, \\n\",\n    \"        \u0027models\u0027:get_models(),\\n\",\n    \"        \u0027norm\u0027:True, \\n\",\n    \"        \u0027train_window\u0027:24, \\n\",\n    \"        \u0027get_acc\u0027: get_acc,\\n\",\n    \"        \u0027resume\u0027:True, \\n\",\n    \"        \u0027num_epoch\u0027:10\\n\",\n    \"    },\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for experiment in experiments:\\n\",\n    \"    exp_log \u003d run_experiment(**experiment)\\n\",\n    \"    print(exp_log)\\n\",\n    \"    experiment_filepath \u003d os.path.join(\\n\",\n    \"                        experiment[\u0027experiment_path\u0027], \\n\",\n    \"                        experiment[\u0027model_root\u0027],\\n\",\n    \"                        \u0027norm\u0027 if experiment[\u0027norm\u0027] else \u0027nonorm\u0027,\\n\",\n    \"                        str(experiment[\u0027train_window\u0027]),\\n\",\n    \"                        \u0027exp_log.pkl\u0027)\\n\",\n    \"    save_pickle(exp_log, experiment_filepath)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.5.2\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}