{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extending PyTorch\n",
    "\n",
    "In this note we’ll cover ways of extending torch.nn, torch.autograd, and writing custom C extensions utilizing our C libraries.\n",
    "\n",
    "Extending torch.autograd\n",
    "Adding operations to autograd requires implementing a new Function subclass for each operation. Recall that Function s are what autograd uses to compute the results and gradients, and encode the operation history. Every new function requires you to implement 2 methods:\n",
    "\n",
    "forward() - the code that performs the operation. It can take as many arguments as you want, with some of them being optional, if you specify the default values. All kinds of Python objects are accepted here. Variable arguments will be converted to Tensor s before the call, and their use will be registered in the graph. Note that this logic won’t traverse lists/dicts/any other data structures and will only consider Variables that are direct arguments to the call. You can return either a single Tensor output, or a tuple of Tensor s if there are multiple outputs. Also, please refer to the docs of Function to find descriptions of useful methods that can be called only from forward().\n",
    "backward() - gradient formula. It will be given as many Variable arguments as there were outputs, with each of them representing gradient w.r.t. that output. It should return as many Variable s as there were inputs, with each of them containing the gradient w.r.t. its corresponding input. If your inputs didn’t require gradient (see needs_input_grad), or were non-Variable objects, you can return None. Also, if you have optional arguments to forward() you can return more gradients than there were inputs, as long as they’re all None.\n",
    "Below you can find code for a Linear function from torch.nn, with additional comments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size:  torch.Size([2, 4, 3])\nunsqueezed(0) input:  torch.Size([1, 2, 4, 3])\nunsqueezed(1) input:  torch.Size([2, 1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# unsqueeze() inserts singleton dim at position given as parameter\n",
    "input = torch.Tensor(2, 4, 3)\n",
    "print(\"input size: \", input.size())\n",
    "print(\"unsqueezed(0) input: \", input.unsqueeze(0).size())\n",
    "print(\"unsqueezed(1) input: \", input.unsqueeze(1).size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
